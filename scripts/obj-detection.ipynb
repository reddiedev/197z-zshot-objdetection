{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEE 197z Project 1 - Zero Shot Object Detection\n",
    "use SAM to perform zero-shot object detection using COCO 2017 val split. \n",
    "\n",
    "*Author: Sean Red Mendoza | 2020-01751 | scmendoza5@up.edu.ph*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
    "import os\n",
    "\n",
    "import open_clip\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "import json\n",
    "import shutil\n",
    "import urllib\n",
    "\n",
    "import cv2\n",
    "import urllib.request\n",
    "import validators\n",
    "from pytriton.client import ModelClient\n",
    "\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toggle on verbose logging for some of the functions\n",
    "VERBOSE_LOGGING = True\n",
    "\"\"\"\n",
    "bool: determine if the program outputs debug info\n",
    "\"\"\"\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:256\"\n",
    "print(\"[setup]: determining CUDA support...\")\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Torchvision version:\", torchvision.__version__)\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean output directory\n",
    "def clean_output():\n",
    "    \"\"\"\n",
    "    deletes the contents of the output directory, if it exists\n",
    "    create output folder, if none exists\n",
    "    \"\"\"\n",
    "    if not os.path.exists(\"../output\"):\n",
    "        os.mkdir(f\"../output\")\n",
    "    shutil.rmtree(\"../output/\")\n",
    "    os.mkdir(f\"../output/\")\n",
    "    \n",
    "clean_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_checkpoint = os.path.join(\"../checkpoints\", \"sam_vit_h_4b8939.pth\")\n",
    "print(\"[sam]: loading checkpoint file...\")\n",
    "# Download the file if it does not exist\n",
    "url = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\"\n",
    "if not os.path.isfile(sam_checkpoint):\n",
    "    urllib.request.urlretrieve(url, sam_checkpoint)\n",
    "\n",
    "print(\"[sam]: loading segment_anything model...\")\n",
    "model_type = \"vit_h\"\n",
    "device_one = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam = sam.to(device=device_one)\n",
    "mask_generator = SamAutomaticMaskGenerator(\n",
    "    model=sam,\n",
    "    points_per_side=32,\n",
    "    points_per_batch=128,\n",
    "    pred_iou_thresh=0.8,\n",
    "    box_nms_thresh=0, # prevent overlapping of masks\n",
    "    stability_score_thresh = -0.4, # allow masks to cover \"entire objects\"\n",
    "    crop_nms_thresh=0.8,\n",
    "    crop_n_layers=1,\n",
    "    crop_n_points_downscale_factor=2,\n",
    "    min_mask_region_area=4000,  # Requires open-cv to run post-processing,\n",
    ")\n",
    "print(\"[sam]: done loading segment_anything model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"[0]: loading coco annotations and captions...\")\n",
    "dataDir = '../coco'\n",
    "dataType = 'val2017'\n",
    "annFile = '{}/annotations/instances_{}.json'.format(dataDir, dataType)\n",
    "\n",
    "if not os.path.isfile(annFile):\n",
    "    print(\"please download the coco val2017 instances dataset!\")\n",
    "    print(\"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\")\n",
    "    quit()\n",
    "\n",
    "# initialize COCO api for instance annotations + COCO ground truth api\n",
    "cocoGt = COCO(annFile)\n",
    "annFile = '{}/annotations/captions_{}.json'.format(dataDir, dataType)\n",
    "coco_caps = COCO(annFile)\n",
    "catIDs = cocoGt.getCatIds()\n",
    "cats = cocoGt.loadCats(catIDs)\n",
    "cocoCategories = [cat['name'] for cat in cats]\n",
    "        \n",
    "paperlabels_filename = \"../labels/coco-labels-paper.txt\"  \n",
    "if not os.path.isfile(paperlabels_filename):\n",
    "    print(\"please download the coco labels!\")\n",
    "    print(\"https://raw.githubusercontent.com/amikelive/coco-labels/master/coco-labels-paper.txt\")\n",
    "    file_url = \"https://raw.githubusercontent.com/amikelive/coco-labels/master/coco-labels-paper.txt\"\n",
    "    urllib.request.urlretrieve(file_url, paperlabels_filename)\n",
    "    \n",
    "with open(paperlabels_filename, \"r\") as file:\n",
    "    lines = file.readlines()  \n",
    "cocopaper_labels = [line.strip() for line in lines]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"[open-clip]: creating open clip model...\")\n",
    "modelType = 'ViT-B-32'\n",
    "modelDataset = \"laion2b_s34b_b79k\"\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    modelType, pretrained=modelDataset)\n",
    "tokenizer = open_clip.get_tokenizer(modelType)\n",
    "\n",
    "device_two = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.device_count() >= 2:\n",
    "    device_two = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device_two)\n",
    "\n",
    "print(\"[open-clip]: done open clip model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_labels = []\n",
    "\"\"\"\n",
    "string[]: list of labels to be used by OpenClip Model\n",
    "\"\"\"\n",
    "\n",
    "print(\"[open-clip]: generating openclip labels dataset...\")\n",
    "# use coco paper categories as labels\n",
    "model_labels.extend(cocopaper_labels)\n",
    "\n",
    "# use SIMPLE IMAGENET LABELS\n",
    "\n",
    "filename = \"../labels/imagenet-simple-labels.json\"\n",
    "url = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n",
    "\n",
    "# Download the file if it does not exist\n",
    "if not os.path.isfile(filename):\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "with open(filename) as f:\n",
    "    raw_imagenet_labels = eval(f.read())\n",
    "imagenet_labels = []\n",
    "for imagenet_label in raw_imagenet_labels:\n",
    "    imagenet_labels.append(imagenet_label.lower().strip())\n",
    "# if input_mode == \"manual\":\n",
    "#     model_labels.extend(imagenet_labels)\n",
    "\n",
    "\n",
    "# use Open Images Classes Dataset \n",
    "# from https://storage.googleapis.com/openimages/2017_11/classes_2017_11.tar.gz\n",
    "\n",
    "filter_filename = \"../labels/classes-bbox.txt\"  \n",
    "with open(filter_filename, \"r\") as file:\n",
    "    lines = file.readlines()  \n",
    "filter_classes = [line.strip() for line in lines]\n",
    "labels_filename = '../labels/openimages-classes-raw.csv'\n",
    "openimage_data = []\n",
    "with open(labels_filename, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        if (row[0] in filter_classes):\n",
    "            openimage_data.append(row[1])\n",
    "\n",
    "column_index = 1  \n",
    "openimage_labels = [word.lower().strip() for word in openimage_data]\n",
    "# if input_mode == \"manual\":\n",
    "#     model_labels.extend(openimage_labels)\n",
    "\n",
    "# PROCESS LABELS\n",
    "# model_labels.append(\"unknown\")\n",
    "model_labels = list(set(model_labels)) # remove duplicates from wordlist\n",
    "if VERBOSE_LOGGING:\n",
    "    pprint(model_labels)\n",
    "text = tokenizer(model_labels)\n",
    "text = text.to(device_two)\n",
    "\n",
    "print(\"[open-clip]: done generating openclip labels dataset!\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images\n",
    "This program supports manual entry of input image link/file path or random sampling from the CoCo 2017 Validation Dataset\n",
    "*Please select your desired input method below:*\n",
    "\n",
    "- [1] Enter Input Image Link/ File Path\n",
    "- [2] Leave blank for Random CoCo Image from the 2017 Validation Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have selected a RANDOM image analysis from the CoCo 2017 dataset\n"
     ]
    }
   ],
   "source": [
    "user_input = \"https://djl.ai/examples/src/test/resources/dog_bike_car.jpg\"\n",
    "\"\"\"\n",
    "string: stores image file path or link from user input\n",
    "\"\"\"\n",
    "\n",
    "def get_valid_input():\n",
    "    \"\"\" \n",
    "    gets a valid image source from user input\n",
    "    \n",
    "    - manual image link/ file path\n",
    "    - leave blank for random 1 image\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        user_input = input(\"Enter image link (leave blank for random image):\")\n",
    "        if user_input == \"\":\n",
    "            return \"random\"\n",
    "        elif validators.url(user_input):\n",
    "            return \"manual\"\n",
    "        else:\n",
    "            print(\"File path entered, please ensure image exists at provided file path\")\n",
    "            \n",
    "\n",
    "input_mode = get_valid_input()\n",
    "\"\"\"\n",
    "- manual: user provided image file path or link\n",
    "- random: program will get random image from coco 2017 validation dataset\n",
    "\"\"\"\n",
    "\n",
    "if input_mode == \"manual\":\n",
    "    print(f\"You have selected MANUAL image analysis of {user_input}\")\n",
    "elif input_mode == \"random\":\n",
    "    print(f\"You have selected a RANDOM image analysis from the CoCo 2017 dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_category_ID = 0\n",
    "image_ID = 0\n",
    "image_area = 0\n",
    "image_link = \"\"\n",
    "image_ground_truth_labels = []\n",
    "\n",
    "def get_random_coco_image(n):\n",
    "    \"\"\"\n",
    "    gets random image/s from the coco 2017 validation dataset\n",
    "\n",
    "    args:\n",
    "        n (int): number of images to retrieve\n",
    "        \n",
    "    returns:\n",
    "        imgID (int): coco image id\n",
    "        catID (int): coco category ID (randomly generated)\n",
    "        imgURL (string): link to the image\n",
    "        imgArea (float): image pixel area\n",
    "        ground_truth_labels (string[]): ground truth labels for image from coco captions\n",
    "\n",
    "    \"\"\"\n",
    "    for i in range(n):\n",
    "        # get random coco image\n",
    "        catID = catIDs[np.random.randint(0, len(catIDs))]\n",
    "        catImages = cocoGt.getImgIds(catIds=[catID])\n",
    "        imgID = catImages[np.random.randint(0, len(catImages))]\n",
    "        # build image meta data\n",
    "        img = cocoGt.loadImgs(imgID)[0]\n",
    "        imgArea = img['height']*img['width']\n",
    "        imgURL = img['coco_url']\n",
    "        # fetch ground truth labels from coco\n",
    "        annIds = coco_caps.getAnnIds(imgIds=imgID)\n",
    "        anns = coco_caps.loadAnns(annIds)\n",
    "        ground_truth_labels = list(map(lambda x: x['caption'], anns))\n",
    "        return imgID, catID, imgURL, imgArea, ground_truth_labels\n",
    "    \n",
    "\n",
    "if input_mode == \"random\":\n",
    "    image_ID, image_category_ID, image_link, image_area, image_ground_truth_labels = get_random_coco_image(1)\n",
    "elif input_mode == \"manual\":\n",
    "    image_link = user_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_results = []\n",
    "label_results = []\n",
    "top_one_score = 0\n",
    "top_five_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_anns(anns):\n",
    "    \"\"\" \n",
    "    writes annotations/masks on image in matplotlib figure\n",
    "    \n",
    "    args:\n",
    "        anns (SAMMask[]): list of SAM Masks\n",
    "    \"\"\"\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        img = np.ones((m.shape[0], m.shape[1], 3))\n",
    "        color_mask = np.random.random((1, 3)).tolist()[0]\n",
    "        for i in range(3):\n",
    "            img[:, :, i] = color_mask[i]\n",
    "        ax.imshow(np.dstack((img, m*0.35)))\n",
    "        \n",
    "def shows_masks_with_labels(anns, labels, probs):\n",
    "    \"\"\" \n",
    "    writes masks on image in matplotlib figure and annotates label with probability\n",
    "    \n",
    "    args:\n",
    "        anns (SAMMask[]): list of SAM Masks,\n",
    "        labels (string[]): labels for each mask,\n",
    "        probs (float[0,1][]): probability for each label\n",
    "    \"\"\"\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "    MINIMUM_MASK_PROB = 0.51\n",
    "    for i in range(len(sorted_anns)):\n",
    "        mask = sorted_anns[i]\n",
    "        label = labels[i]\n",
    "        prob = probs[i]\n",
    "        if (prob >= MINIMUM_MASK_PROB):\n",
    "            x, y, w, h = mask['bbox']\n",
    "            color = np.random.rand(3,)\n",
    "            rect = plt.Rectangle((x, y), w, h, fill=False, color=color)\n",
    "            ax.add_patch(rect)\n",
    "            color = np.append(color, 0.5)\n",
    "            ax.text(x, y, f\"{label} {prob:.2f}\", color='black', fontsize=10, bbox=dict(facecolor=color, alpha=0.5))\n",
    "\n",
    "\n",
    "def generate_masks(image):\n",
    "    \"\"\"\n",
    "    automatically generates masks for a given image using SAM AutomaticMaskGenerator\n",
    "    \n",
    "    args:\n",
    "        image: image file\n",
    "    \n",
    "    returns: \n",
    "        list of masks\n",
    "    \"\"\"\n",
    "    raw_masks = mask_generator.generate(image)\n",
    "    for i in range(len(raw_masks)):\n",
    "        mask = raw_masks[i]\n",
    "        if VERBOSE_LOGGING:\n",
    "            print(f\"[mask-{i+1}]: predicted_iou: {mask['predicted_iou']:.2f}, stability score: {mask['stability_score']:.2f}\")\n",
    "    sorted_raw_masks = sorted(raw_masks, key=(lambda x: x['area']), reverse=True)\n",
    "    return sorted_raw_masks\n",
    "\n",
    "def filter_masks(raw_masks, area):\n",
    "    \"\"\"\n",
    "    removes undesired masks for better object detection performance\n",
    "    \n",
    "    args:\n",
    "        raw_masks (SAMMask[]): list of SAM Masks,\n",
    "        area (float): minimum image area (pixels) \n",
    "        \n",
    "    returns:\n",
    "        filtered_masks (SAMMask[]): filtered list of SAM Masks\n",
    "    \"\"\"\n",
    "    if len(raw_masks) == 0:\n",
    "        return\n",
    "    length = len(raw_masks)\n",
    "    sorted_anns = sorted(raw_masks, key=(lambda x: x['area']), reverse=True)\n",
    "    filtered_masks = []\n",
    "    for i in range(length):\n",
    "        mask = sorted_anns[i]\n",
    "        if (mask['area'] < area):\n",
    "            continue\n",
    "        filtered_masks.append(mask)\n",
    "    return filtered_masks\n",
    "\n",
    "\n",
    "def save_raw_mask_images(image, raw_masks):\n",
    "    \"\"\"\n",
    "    crops source image from generated raw masks and saves each as .jpg files\n",
    "    \n",
    "    args:\n",
    "        image: image file,\n",
    "        raw_masks (SAMMask[]): list of raw SAM Masks\n",
    "    \"\"\"\n",
    "    if len(raw_masks) == 0:\n",
    "        return\n",
    "    length = len(raw_masks)\n",
    "    sorted_anns = sorted(raw_masks, key=(lambda x: x['area']), reverse=True)\n",
    "    counter = 1\n",
    "    for i in range(length):\n",
    "        mask = sorted_anns[i]\n",
    "        x, y, w, h = mask['bbox']\n",
    "        x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "        im = image[y:y+h, x:x+w]\n",
    "        plt.figure(figsize=(20, 20))\n",
    "        plt.imshow(im)\n",
    "        plt.axis('off')\n",
    "        plt.savefig(f\"../output/raw-mask-{counter}.jpg\",\n",
    "                    bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "        counter += 1\n",
    "        \n",
    "def save_mask_images(image, filtered_masks):\n",
    "    \"\"\"\n",
    "    crops source image from generated filered masks and saves each as .jpg files\n",
    "    \n",
    "    args:\n",
    "        image: image file,\n",
    "        filtered_masks (SAMMask[]): list of filtered SAM Masks\n",
    "    \"\"\"\n",
    "    if len(filtered_masks) == 0:\n",
    "        return\n",
    "    length = len(filtered_masks)\n",
    "    sorted_anns = sorted(filtered_masks, key=(lambda x: x['area']), reverse=True)\n",
    "    counter = 1\n",
    "    for i in range(length):\n",
    "        mask = sorted_anns[i]\n",
    "        x, y, w, h = mask['bbox']\n",
    "        x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "        im = image[y:y+h, x:x+w]\n",
    "        plt.figure(figsize=(20, 20))\n",
    "        plt.imshow(im)\n",
    "        plt.axis('off')\n",
    "        plt.savefig(f\"../output/mask-{counter}.jpg\",\n",
    "                    bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "        counter += 1\n",
    "\n",
    "def generate_labels(anns):\n",
    "    \"\"\"\n",
    "    generates labels for each mask using OpenClip \n",
    "    \n",
    "    args:\n",
    "        anns (SAMMask[]): list of masks\n",
    "        \n",
    "    returns:\n",
    "        labels (string[]): labels for each mask,\n",
    "        probs (float[0,1][]): probability for each label\n",
    "    \"\"\"\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    length = len(anns)\n",
    "    labels = []\n",
    "    probs =  []\n",
    "    for i in range(length):\n",
    "        mask = anns[i]\n",
    "        im = Image.open(f\"../output/mask-{i+1}.jpg\").convert(\"RGB\")\n",
    "        img = preprocess(im).unsqueeze(0)\n",
    "        img = img.to(device_two)\n",
    "\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            image_features = model.encode_image(img)\n",
    "            text_features = model.encode_text(text)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            text_probs = (100.0 * image_features @\n",
    "                          text_features.T).softmax(dim=-1)\n",
    "\n",
    "        text_prob = np.max(text_probs.cpu().numpy())\n",
    "        index = np.argmax(text_probs.cpu().numpy())\n",
    "        label = model_labels[index]\n",
    "        sorted_probs = np.argsort(text_probs.cpu().numpy(), axis=None)\n",
    "        top_sorted_probs = sorted_probs[:5]\n",
    "        other_labels = []\n",
    "        for j in range(5):\n",
    "            test_label_index = top_sorted_probs[j]\n",
    "            test_label = model_labels[test_label_index]\n",
    "            other_labels.append(test_label)\n",
    "        other_labels_text = \" \".join(other_labels)\n",
    "        \n",
    "        TEXT_PROB_THRESHOLD = 0.8\n",
    "        if input_mode == \"random\" and label in cocopaper_labels and text_prob >= TEXT_PROB_THRESHOLD:\n",
    "            image_category = cocopaper_labels.index(label)\n",
    "            result = {'image_id': image_ID, 'category_id': image_category+1,\n",
    "                    \"bbox\": mask['bbox'], \"score\": mask['predicted_iou']}\n",
    "            coco_results.append(result)\n",
    "\n",
    "        labels.append(label)\n",
    "        probs.append(text_prob)\n",
    "        if VERBOSE_LOGGING:\n",
    "            print(f\"[{i+1}/{length}]:\",  label, f\"({text_prob*100:.2f}%)\", other_labels_text)\n",
    "    return labels, probs\n",
    "        \n",
    "\n",
    "def save_labels(anns, labels, probs):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    length = len(anns)\n",
    "    values = []\n",
    "    for i in range(length):\n",
    "        mask = anns[i]\n",
    "        label = labels[i]\n",
    "        text_prob = probs[i]\n",
    "        values.append(\n",
    "            {\"label\": label, \"area\": mask[\"area\"], \"prob\": text_prob})\n",
    "        \n",
    "    # generate top 5 labels according to label_accuracy and mask_area\n",
    "    sorted_values = sorted(values, key=lambda x: x['prob'], reverse=True)\n",
    "    payload = sorted_values[:5]\n",
    "    labels = list(map(lambda d: d['label'], payload))\n",
    "    return labels\n",
    "\n",
    "def isTopNAccurate(substrings, string):\n",
    "    results = []\n",
    "    for substring in substrings:\n",
    "        if substring.lower().strip() in string.lower().strip():\n",
    "            results.append(True)\n",
    "        else:\n",
    "            results.append(False)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://202.92.132.48:8000/\"\n",
    "def infer_model(url=url, image=\"../images/dog_car.jpg\", model=\"Yolov8x\"):\n",
    "    with ModelClient(url, model) as client:\n",
    "        if validators.url(image):\n",
    "            with urllib.request.urlopen(image) as url_response:\n",
    "                img_array = np.array(bytearray(url_response.read()), dtype=np.uint8)\n",
    "                image = cv2.imdecode(img_array, -1)\n",
    "        else:\n",
    "            image = cv2.imread(image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        outputs = client.infer_sample(image)\n",
    "        for k, v in outputs.items():\n",
    "            if k == \"names\":\n",
    "                names = v.tobytes().decode('utf-32').split(\"|\")\n",
    "                names = names[:-1]\n",
    "            elif k == \"bboxes\":\n",
    "                bboxes = v\n",
    "            elif k == \"probs\":\n",
    "                probs = v\n",
    "        \n",
    "        # Plotting\n",
    "        fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "        ax.imshow(image)\n",
    "        ax.axis('off')\n",
    "        for i in range(len(names)):\n",
    "            if probs[i] > 0.5:\n",
    "                x1, y1, x2, y2 = bboxes[i]\n",
    "                w = x2 - x1\n",
    "                h = y2 - y1\n",
    "                # make the color random\n",
    "                color = np.random.rand(3,)\n",
    "                rect = plt.Rectangle((x1, y1), w, h, fill=False, color=color)\n",
    "                ax.add_patch(rect)\n",
    "                # use the color and add transparency of 0.5\n",
    "                color = np.append(color, 0.5)\n",
    "                # add text with white background\n",
    "                ax.text(x1, y1, f\"{names[i]} {probs[i]:.2f}\", color='black', fontsize=10, bbox=dict(facecolor=color, alpha=0.5))\n",
    "                     \n",
    "        fig.savefig(f\"../output/yolo-masks-labels.jpg\",\n",
    "            bbox_inches='tight', pad_inches=0)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_output() # create output folder for this image\n",
    "\n",
    "image = io.imread(image_link)\n",
    "height, width = image.shape[:2]\n",
    "imageArea = height * width\n",
    "maskArea = 0.0025 * imageArea\n",
    "\n",
    "# save original image for future analysis\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.savefig(f\"../output/source.jpg\",\n",
    "            bbox_inches='tight', pad_inches=0)\n",
    "plt.close()\n",
    "\n",
    "raw_masks = generate_masks(image)    \n",
    "if VERBOSE_LOGGING:\n",
    "    print(f\"generated {len(raw_masks)} masks...\")\n",
    "    \n",
    "filtered_masks = filter_masks(raw_masks, maskArea)\n",
    "if VERBOSE_LOGGING:\n",
    "    print(f\"masks filtered down to {len(filtered_masks)} masks...\")\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "\n",
    "if VERBOSE_LOGGING:\n",
    "    print(\"applying raw masks on source image...\")\n",
    "\n",
    "show_anns(raw_masks)\n",
    "plt.axis('off')\n",
    "plt.savefig(f\"../output/generated-raw-masks.jpg\",\n",
    "            bbox_inches='tight', pad_inches=0)\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "\n",
    "if VERBOSE_LOGGING:\n",
    "    print(\"applying filtered masks on source image...\")\n",
    "    \n",
    "show_anns(filtered_masks)\n",
    "plt.axis('off')\n",
    "plt.savefig(f\"../output/generated-filtered-masks.jpg\",\n",
    "            bbox_inches='tight', pad_inches=0)\n",
    "plt.close()\n",
    "\n",
    "source_image = io.imread(f\"../output/source.jpg\")\n",
    "fig, axs = plt.subplots(1, 3,figsize=(30, 10))\n",
    "axs[0].imshow(source_image, cmap='gray')\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title(\"Source Image\")\n",
    "masked_image = io.imread(f\"../output/generated-raw-masks.jpg\")\n",
    "axs[1].imshow(masked_image, cmap='gray')\n",
    "axs[1].axis('off')\n",
    "axs[1].set_title(\"Image with Raw Masks\")\n",
    "masked_image = io.imread(f\"../output/generated-filtered-masks.jpg\")\n",
    "axs[2].imshow(masked_image, cmap='gray')\n",
    "axs[2].axis('off')\n",
    "axs[2].set_title(\"Image with Filtered Masks\")\n",
    "plt.subplots_adjust(wspace=0.1)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "if VERBOSE_LOGGING:\n",
    "    print(\"saving cropped mask image files...\")\n",
    "# save mask image files\n",
    "# save_raw_mask_images(image, raw_masks)\n",
    "save_mask_images(image, filtered_masks)\n",
    "\n",
    "# print(f\"({image_index}): generating labels...\")\n",
    "generated_labels, generated_probs = generate_labels(filtered_masks)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "shows_masks_with_labels(filtered_masks, generated_labels, generated_probs)\n",
    "plt.axis('off')\n",
    "plt.savefig(f\"../output/generated-masks-labels.jpg\",\n",
    "            bbox_inches='tight', pad_inches=0)\n",
    "plt.close()\n",
    "\n",
    "infer_model(image=image_link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_generated_labels = list(set(generated_labels)) # removes duplicates, if any\n",
    "\n",
    "if input_mode == \"random\":\n",
    "    if VERBOSE_LOGGING:\n",
    "        print(\"currently evaluating CoCo w/ image ID:\", image_ID, \"...\")\n",
    "    annIds = coco_caps.getAnnIds(imgIds=image_ID)\n",
    "    anns = coco_caps.loadAnns(annIds)\n",
    "    ground_truth_labels = []\n",
    "    for ann in anns:\n",
    "        ground_truth_labels.append(ann['caption'])\n",
    "    ground_truth_string = \" \".join(ground_truth_labels)\n",
    "\n",
    "    # for each mask image, annotate using open-clip\n",
    "    sorted_labels = save_labels(filtered_masks, generated_labels, generated_probs)    \n",
    "    top_one_score = sorted_labels[0] in ground_truth_string\n",
    "    top_five_scores = isTopNAccurate(\n",
    "        sorted_labels, ground_truth_string)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (input_mode == \"random\"):    \n",
    "    json_object = json.dumps(coco_results, indent=4)\n",
    "    with open(\"../output/results.json\", \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "    cocoDt = cocoGt.loadRes(\"../output/results.json\")\n",
    "    cocoEval = COCOeval(cocoGt, cocoDt, 'bbox')\n",
    "    cocoEval.params.imgIds = [image_ID]\n",
    "    cocoEval.evaluate()\n",
    "    cocoEval.accumulate()\n",
    "    cocoEval.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GENERATED LABELS\")\n",
    "pprint(clean_generated_labels)\n",
    "if (input_mode == \"random\"):\n",
    "    print(\"GROUND TRUTH LABELS\")\n",
    "    pprint(ground_truth_labels)\n",
    "\n",
    "if (input_mode == \"random\"):  \n",
    "    print(\"-------------------------\")\n",
    "    print(\"Top-1 Label Accuracy SCORE:\")\n",
    "    print(f\"{top_one_score*100:.2f}%\")\n",
    "    print(\"Top-5 Label Accuracy SCORE:\")\n",
    "    top_five_score = top_five_scores.count(True)/len(top_five_scores)\n",
    "    print(f\"{top_five_score*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generated_image = io.imread(f\"../output/generated-masks-labels.jpg\")\n",
    "fig, axs = plt.subplots(1, 2,figsize=(20, 10))\n",
    "axs[0].imshow(generated_image, cmap='gray')\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title(\"Generated Image (SAM-OpenClip)\")\n",
    "yolo_image = io.imread(f\"../output/yolo-masks-labels.jpg\")\n",
    "axs[1].imshow(yolo_image, cmap='gray')\n",
    "axs[1].axis('off')\n",
    "axs[1].set_title(\"Generated Image (Yolov8)\")\n",
    "plt.subplots_adjust(wspace=0.1)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_mode == \"random\":\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image); plt.axis('off')\n",
    "        annIds = cocoGt.getAnnIds(imgIds=image_ID, catIds=catIDs, iscrowd=None)\n",
    "        anns = cocoGt.loadAnns(annIds)\n",
    "        cocoGt.showAnns(anns)\n",
    "        plt.savefig(f\"../output/coco-ground-truth.jpg\",\n",
    "                bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "        \n",
    "        coco_gt_image = io.imread(f\"../output/coco-ground-truth.jpg\")\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(30, 10))\n",
    "        axs[0].imshow(coco_gt_image, cmap='gray')\n",
    "        axs[0].axis('off')\n",
    "        axs[0].set_title(\"CoCo Ground Truth\")\n",
    "        axs[1].imshow(generated_image, cmap='gray')\n",
    "        axs[1].axis('off')\n",
    "        axs[1].set_title(\"Generated Image (SAM-OpenClip)\")\n",
    "        axs[2].imshow(yolo_image, cmap='gray')\n",
    "        axs[2].axis('off')\n",
    "        axs[2].set_title(\"Generated Image (Yolov8)\")\n",
    "        plt.subplots_adjust(wspace=0.01)\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
