{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEE 197z Project 1 - Zero Shot Object Detection\n",
    "use SAM to perform zero-shot object detection using COCO 2017 val split. \n",
    "\n",
    "*Author: Sean Red Mendoza | 2020-01751 | scmendoza5@up.edu.ph*\n",
    "\n",
    "## Tools/ References\n",
    "- [SegmentAnything](https://github.com/facebookresearch/segment-anything)\n",
    "- [OpenClip](https://github.com/mlfoundations/open_clip)\n",
    "- [Coco 2017 Validation Dataset](https://cocodataset.org/#home)\n",
    "- [roatienza/mlops](https://github.com/roatienza/mlops)\n",
    "- [roatienza/Deep-Learning-Experiments](https://github.com/roatienza/Deep-Learning-Experiments)\n",
    "- [Google Cloud G2 GPU VM (Nvidia L4)](https://cloud.google.com/blog/products/compute/introducing-g2-vms-with-nvidia-l4-gpus)\n",
    "\n",
    "## Goals\n",
    "\n",
    "## Approach\n",
    "\n",
    "## Notes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
    "import os\n",
    "import open_clip\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "import json\n",
    "import shutil\n",
    "import re\n",
    "import urllib\n",
    "\n",
    "import cv2\n",
    "import urllib.request\n",
    "import validators\n",
    "from pytriton.client import ModelClient\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toggle on verbose logging for some of the functions\n",
    "VERBOSE_LOGGING = False\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:256\"\n",
    "print(\"[setup]: determining CUDA support...\")\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Torchvision version:\", torchvision.__version__)\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean output directory\n",
    "if not os.path.exists(\"../output\"):\n",
    "    os.mkdir(f\"../output\")\n",
    "shutil.rmtree(\"../output/\")\n",
    "os.mkdir(f\"../output/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images\n",
    "This program supports manual entry of input image links or random sampling from the CoCo 2017 Validation Dataset\n",
    "*Please select your desired input method below:*\n",
    "\n",
    "- [1] Input Images\n",
    "- [2] CoCo Images\n",
    "\n",
    "After selecting the input image source, you will be asked to input the following:\n",
    "- [a] List of image URL strings, or\n",
    "- [b] number of random images sourced from CoCo 2017*\n",
    "\n",
    "**a hard limit of 10 images is set to prevent overloading of the system*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"../images/dog_car.jpg\"\n",
    "\n",
    "def get_valid_input():\n",
    "    while True:\n",
    "        user_input = input(\"Enter image link (leave blank for random image):\")\n",
    "        if user_input == \"\":\n",
    "            return \"random\"\n",
    "        elif validators.url(user_input):\n",
    "            return \"manual\"\n",
    "        else:\n",
    "            print(\"Invalid input. Please enter a valid image link.\")\n",
    "            \n",
    "\n",
    "input_mode = get_valid_input()\n",
    "\n",
    "if input_mode == \"manual\":\n",
    "    print(f\"You have selected MANUAL image analysis of {user_input}\")\n",
    "elif input_mode == \"random\":\n",
    "    print(f\"You have selected a RANDOM image analysis from the CoCo 2017 dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"[0]: loading coco annotations and captions...\")\n",
    "dataDir = '../coco'\n",
    "dataType = 'val2017'\n",
    "annFile = '{}/annotations/instances_{}.json'.format(dataDir, dataType)\n",
    "\n",
    "if not os.path.isfile(annFile):\n",
    "    print(\"please download the coco val2017 instances dataset!\")\n",
    "    print(\"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\")\n",
    "    quit()\n",
    "\n",
    "# initialize COCO api for instance annotations + COCO ground truth api\n",
    "cocoGt = COCO(annFile)\n",
    "annFile = '{}/annotations/captions_{}.json'.format(dataDir, dataType)\n",
    "coco_caps = COCO(annFile)\n",
    "catIDs = cocoGt.getCatIds()\n",
    "cats = cocoGt.loadCats(catIDs)\n",
    "cocoCategories = [cat['name'] for cat in cats]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_category_ID = 0\n",
    "image_ID = 0\n",
    "image_area = 0\n",
    "image_link = \"\"\n",
    "image_ground_truth_labels = []\n",
    "\n",
    "def get_random_coco_image(n):\n",
    "    for i in range(n):\n",
    "        # get random coco image\n",
    "        catID = catIDs[np.random.randint(0, len(catIDs))]\n",
    "        catImages = cocoGt.getImgIds(catIds=[catID])\n",
    "        imgID = catImages[np.random.randint(0, len(catImages))]\n",
    "        # build image meta data\n",
    "        img = cocoGt.loadImgs(imgID)[0]\n",
    "        imgArea = img['height']*img['width']\n",
    "        imgURL = img['coco_url']\n",
    "        # fetch ground truth labels from coco\n",
    "        annIds = coco_caps.getAnnIds(imgIds=imgID)\n",
    "        anns = coco_caps.loadAnns(annIds)\n",
    "        ground_truth_labels = list(map(lambda x: x['caption'], anns))\n",
    "        return imgID, catID, imgURL, imgArea, ground_truth_labels\n",
    "    \n",
    "\n",
    "if input_mode == \"random\":\n",
    "    image_ID, image_category_ID, image_link, image_area, image_ground_truth_labels = get_random_coco_image(1)\n",
    "elif input_mode == \"manual\":\n",
    "    image_link = user_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_results = []\n",
    "label_results = []\n",
    "top_one_scores = []\n",
    "top_five_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_checkpoint = os.path.join(\"../checkpoints\", \"sam_vit_h_4b8939.pth\")\n",
    "print(\"[sam]: loading checkpoint file...\")\n",
    "# Download the file if it does not exist\n",
    "url = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\"\n",
    "if not os.path.isfile(sam_checkpoint):\n",
    "    urllib.request.urlretrieve(url, sam_checkpoint)\n",
    "\n",
    "print(\"[sam]: loading segment_anything model...\")\n",
    "model_type = \"vit_h\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam = sam.to(device=device)\n",
    "mask_generator = SamAutomaticMaskGenerator(\n",
    "    model=sam,\n",
    "    points_per_side=32,\n",
    "    points_per_batch=128,\n",
    "    pred_iou_thresh=0.6,\n",
    "    box_nms_thresh=0.1, # prevent overlapping of masks\n",
    "    stability_score_thresh=0.2, # allow masks to cover \"entire objects\"\n",
    "    min_mask_region_area=1000,  # Requires open-cv to run post-processing\n",
    "    crop_n_layers=1,\n",
    "    crop_n_points_downscale_factor=2,\n",
    ")\n",
    "print(\"[sam]: done loading segment_anything model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[open-clip]: creating open clip model...\")\n",
    "modelType = 'ViT-bigG-14'\n",
    "modelDataset = \"laion2b_s39b_b160k\"\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    modelType, pretrained=modelDataset)\n",
    "tokenizer = open_clip.get_tokenizer(modelType)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"[open-clip]: done open clip model!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_labels = []\n",
    "\n",
    "print(\"[open-clip]: generating openclip labels dataset...\")\n",
    "# use coco categories as labels\n",
    "model_labels.extend(cocoCategories)\n",
    "\n",
    "# use SIMPLE IMAGENET LABELS\n",
    "\n",
    "filename = \"../labels/imagenet-simple-labels.json\"\n",
    "url = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n",
    "\n",
    "# Download the file if it does not exist\n",
    "if not os.path.isfile(filename):\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "with open(filename) as f:\n",
    "    raw_imagenet_labels = eval(f.read())\n",
    "imagenet_labels = []\n",
    "for imagenet_label in raw_imagenet_labels:\n",
    "    imagenet_labels.append(imagenet_label.lower().strip())\n",
    "# model_labels.extend(imagenet_labels)\n",
    "\n",
    "\n",
    "# use Open Images Classes Dataset \n",
    "# from https://storage.googleapis.com/openimages/2017_11/classes_2017_11.tar.gz\n",
    "\n",
    "filter_filename = \"../labels/classes-bbox.txt\"  \n",
    "with open(filter_filename, \"r\") as file:\n",
    "    lines = file.readlines()  \n",
    "filter_classes = [line.strip() for line in lines]\n",
    "labels_filename = '../labels/openimages-classes-raw.csv'\n",
    "openimage_data = []\n",
    "with open(labels_filename, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        if (row[0] in filter_classes):\n",
    "            openimage_data.append(row[1])\n",
    "\n",
    "column_index = 1  \n",
    "openimage_labels = [word.lower().strip() for word in openimage_data]\n",
    "# model_labels.extend(openimage_labels)\n",
    "\n",
    "# PROCESS LABELS\n",
    "model_labels.append(\"unknown\")\n",
    "model_labels = list(set(model_labels)) # remove duplicates from wordlist\n",
    "print(model_labels)\n",
    "text = tokenizer(model_labels)\n",
    "text = text.to(device)\n",
    "\n",
    "print(\"[open-clip]: done generating openclip labels dataset!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        img = np.ones((m.shape[0], m.shape[1], 3))\n",
    "        color_mask = np.random.random((1, 3)).tolist()[0]\n",
    "        for i in range(3):\n",
    "            img[:, :, i] = color_mask[i]\n",
    "        ax.imshow(np.dstack((img, m*0.35)))\n",
    "        \n",
    "def shows_masks_with_labels(anns, labels, probs):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "    for i in range(len(sorted_anns)):\n",
    "        mask = sorted_anns[i]\n",
    "        label = labels[i]\n",
    "        prob = probs[i]\n",
    "        if (prob > 0.25):\n",
    "            x, y, w, h = mask['bbox']\n",
    "            color = np.random.rand(3,)\n",
    "            rect = plt.Rectangle((x, y), w, h, fill=False, color=color)\n",
    "            ax.add_patch(rect)\n",
    "            color = np.append(color, 0.5)\n",
    "            ax.text(x, y, f\"{label} {prob:.2f}\", color='black', fontsize=10, bbox=dict(facecolor=color, alpha=0.5))\n",
    "\n",
    "def generate_masks(image):\n",
    "    raw_masks = mask_generator.generate(image)\n",
    "    for i in range(len(raw_masks)):\n",
    "        mask = raw_masks[i]\n",
    "        if VERBOSE_LOGGING:\n",
    "            print(f\"[mask-{i+1}]: predicted_iou: {mask['predicted_iou']}, stability score: {mask['stability_score']}\")\n",
    "        if input_mode == \"random\":\n",
    "            result = {'image_id': image_ID, 'category_id': image_category_ID,\n",
    "                    \"bbox\": mask['bbox'], \"score\": mask['stability_score']}\n",
    "            coco_results.append(result)\n",
    "    return raw_masks\n",
    "    \n",
    "\n",
    "def filter_masks(raw_masks, area):\n",
    "    if len(raw_masks) == 0:\n",
    "        return\n",
    "    length = len(raw_masks)\n",
    "    sorted_anns = sorted(raw_masks, key=(lambda x: x['area']), reverse=True)\n",
    "    filtered_masks = []\n",
    "    for i in range(length):\n",
    "        mask = sorted_anns[i]\n",
    "        if (mask['area'] < area):\n",
    "            continue\n",
    "        filtered_masks.append(mask)\n",
    "       \n",
    "    return filtered_masks\n",
    "\n",
    "\n",
    "def save_mask_images(image, filtered_masks):\n",
    "    if len(filtered_masks) == 0:\n",
    "        return\n",
    "    length = len(filtered_masks)\n",
    "    sorted_anns = sorted(filtered_masks, key=(lambda x: x['area']), reverse=True)\n",
    "    counter = 1\n",
    "    filtered_masks = []\n",
    "    for i in range(length):\n",
    "        mask = sorted_anns[i]\n",
    "        x, y, w, h = mask['bbox']\n",
    "        x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "        im = image[y:y+h, x:x+w]\n",
    "        plt.figure(figsize=(20, 20))\n",
    "        plt.imshow(im)\n",
    "        plt.axis('off')\n",
    "        plt.savefig(f\"../output/mask-{counter}.jpg\",\n",
    "                    bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "        counter += 1\n",
    "        filtered_masks.append(mask)\n",
    "    return filtered_masks\n",
    "\n",
    "def generate_labels(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    length = len(anns)\n",
    "    labels = []\n",
    "    probs =  []\n",
    "    for i in range(length):\n",
    "        im = Image.open(f\"../output/mask-{i+1}.jpg\").convert(\"RGB\")\n",
    "        img = preprocess(im).unsqueeze(0)\n",
    "        img = img.to(device)\n",
    "\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            image_features = model.encode_image(img)\n",
    "            text_features = model.encode_text(text)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            text_probs = (100.0 * image_features @\n",
    "                          text_features.T).softmax(dim=-1)\n",
    "\n",
    "        text_prob = np.max(text_probs.cpu().numpy())\n",
    "        index = np.argmax(text_probs.cpu().numpy())\n",
    "        label = model_labels[index]\n",
    "        labels.append(label)\n",
    "        probs.append(text_prob)\n",
    "        if VERBOSE_LOGGING:\n",
    "            print(f\"[{i+1}/{length}]:\",  label, f\"({text_prob*100:.2f}%)\",)\n",
    "        \n",
    "    return labels, probs\n",
    "        \n",
    "\n",
    "def save_labels(anns, labels, probs):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    length = len(anns)\n",
    "    values = []\n",
    "    for i in range(length):\n",
    "        mask = anns[i]\n",
    "        label = labels[i]\n",
    "        text_prob = probs[i]\n",
    "        values.append(\n",
    "            {\"label\": label, \"area\": mask[\"area\"], \"prob\": text_prob})\n",
    "        \n",
    "    # generate top 5 labels according to label_accuracy and mask_area\n",
    "    sorted_values = sorted(values, key=lambda x: x['prob'])\n",
    "    payload = sorted_values[:5]\n",
    "    labels = list(map(lambda d: d['label'], payload))\n",
    "    return labels\n",
    "\n",
    "def isTopNAccurate(substrings, string):\n",
    "    isAccurate = False\n",
    "    for substring in substrings:\n",
    "        if substring in string:\n",
    "            isAccurate = True\n",
    "            break\n",
    "    return isAccurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://202.92.132.48:8000/\"\n",
    "def infer_model(url=url, image=\"../images/dog_car.jpg\", model=\"Yolov8x\"):\n",
    "    with ModelClient(url, model) as client:\n",
    "        if validators.url(image):\n",
    "            with urllib.request.urlopen(image) as url_response:\n",
    "                img_array = np.array(bytearray(url_response.read()), dtype=np.uint8)\n",
    "                image = cv2.imdecode(img_array, -1)\n",
    "        else:\n",
    "            image = cv2.imread(image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        outputs = client.infer_sample(image)\n",
    "        for k, v in outputs.items():\n",
    "            if k == \"names\":\n",
    "                names = v.tobytes().decode('utf-32').split(\"|\")\n",
    "                names = names[:-1]\n",
    "            elif k == \"bboxes\":\n",
    "                bboxes = v\n",
    "            elif k == \"probs\":\n",
    "                probs = v\n",
    "        \n",
    "        # Plotting\n",
    "        fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "        ax.imshow(image)\n",
    "        ax.axis('off')\n",
    "        for i in range(len(names)):\n",
    "            if probs[i] > 0.5:\n",
    "                x1, y1, x2, y2 = bboxes[i]\n",
    "                w = x2 - x1\n",
    "                h = y2 - y1\n",
    "                # make the color random\n",
    "                color = np.random.rand(3,)\n",
    "                rect = plt.Rectangle((x1, y1), w, h, fill=False, color=color)\n",
    "                ax.add_patch(rect)\n",
    "                # use the color and add transparency of 0.5\n",
    "                color = np.append(color, 0.5)\n",
    "                # add text with white background\n",
    "                ax.text(x1, y1, f\"{names[i]} {probs[i]:.2f}\", color='black', fontsize=10, bbox=dict(facecolor=color, alpha=0.5))\n",
    "                     \n",
    "        fig.savefig(f\"../output/yolo-masks-labels.jpg\",\n",
    "            bbox_inches='tight', pad_inches=0)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output folder for this image\n",
    "image_folder_path = f\"../output\"\n",
    "if not os.path.exists(image_folder_path):\n",
    "    os.mkdir(image_folder_path)\n",
    "\n",
    "image = io.imread(image_link)\n",
    "height, width = image.shape[:2]\n",
    "imageArea = height * width\n",
    "maskArea = 0.01 * imageArea\n",
    "\n",
    "# save original image for future analysis\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.savefig(f\"../output/source.jpg\",\n",
    "            bbox_inches='tight', pad_inches=0)\n",
    "plt.close()\n",
    "\n",
    "raw_masks = generate_masks(image)    \n",
    "if VERBOSE_LOGGING:\n",
    "    print(f\"generated {len(raw_masks)} masks...\")\n",
    "    \n",
    "filtered_masks = filter_masks(raw_masks, maskArea)\n",
    "if VERBOSE_LOGGING:\n",
    "    print(f\"masks filtered down to {len(filtered_masks)} masks...\")\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "\n",
    "if VERBOSE_LOGGING:\n",
    "    print(\"applying raw masks on source image...\")\n",
    "\n",
    "show_anns(raw_masks)\n",
    "plt.axis('off')\n",
    "plt.savefig(f\"../output/generated-raw-masks.jpg\",\n",
    "            bbox_inches='tight', pad_inches=0)\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "\n",
    "if VERBOSE_LOGGING:\n",
    "    print(\"applying filtered masks on source image...\")\n",
    "    \n",
    "show_anns(filtered_masks)\n",
    "plt.axis('off')\n",
    "plt.savefig(f\"../output/generated-filtered-masks.jpg\",\n",
    "            bbox_inches='tight', pad_inches=0)\n",
    "plt.close()\n",
    "\n",
    "source_image = io.imread(f\"../output/source.jpg\")\n",
    "fig, axs = plt.subplots(1, 3,figsize=(30, 10))\n",
    "axs[0].imshow(source_image, cmap='gray')\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title(\"Source Image\")\n",
    "masked_image = io.imread(f\"../output/generated-raw-masks.jpg\")\n",
    "axs[1].imshow(masked_image, cmap='gray')\n",
    "axs[1].axis('off')\n",
    "axs[1].set_title(\"Image with Raw Masks\")\n",
    "masked_image = io.imread(f\"../output/generated-filtered-masks.jpg\")\n",
    "axs[2].imshow(masked_image, cmap='gray')\n",
    "axs[2].axis('off')\n",
    "axs[2].set_title(\"Image with Filtered Masks\")\n",
    "plt.subplots_adjust(wspace=0.1)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "if VERBOSE_LOGGING:\n",
    "    print(\"saving cropped mask image files...\")\n",
    "# save mask image files\n",
    "save_mask_images(image, filtered_masks)\n",
    "\n",
    "# print(f\"({image_index}): generating labels...\")\n",
    "generated_labels, generated_probs = generate_labels(filtered_masks)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "shows_masks_with_labels(filtered_masks, generated_labels, generated_probs)\n",
    "plt.axis('off')\n",
    "plt.savefig(f\"../output/generated-masks-labels.jpg\",\n",
    "            bbox_inches='tight', pad_inches=0)\n",
    "plt.close()\n",
    "\n",
    "infer_model(image=image_link)\n",
    "\n",
    "clean_generated_labels = list(set(generated_labels)) # removes duplicates, if any\n",
    "\n",
    "\n",
    "if input_mode == \"random\":\n",
    "    # print(\"image ID:\", image_id, \"category ID:\", image_category_id)\n",
    "    annIds = coco_caps.getAnnIds(imgIds=image_ID)\n",
    "    anns = coco_caps.loadAnns(annIds)\n",
    "    ground_truth_labels = []\n",
    "    ground_truth = []\n",
    "    for ann in anns:\n",
    "        ground_truth_labels.append(ann['caption'])\n",
    "        words = ann['caption'].split()\n",
    "        for word in words:\n",
    "            ground_truth.append(word.lower())\n",
    "    ground_truth_values = list(set(ground_truth))\n",
    "\n",
    "    # for each mask image, annotate using open-clip\n",
    "    save_labels(filtered_masks, generated_labels, generated_probs)\n",
    " \n",
    "    ground_truth_string = \" \".join(ground_truth_labels)\n",
    "    \n",
    "    top_one_scores.append(clean_generated_labels[0] in ground_truth_string)\n",
    "    top_five_scores.append(isTopNAccurate(\n",
    "        clean_generated_labels, ground_truth_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (input_mode == \"random\"):    \n",
    "    json_object = json.dumps(coco_results, indent=4)\n",
    "    with open(\"../output/results.json\", \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "    cocoDt = cocoGt.loadRes(\"../output/results.json\")\n",
    "    cocoEval = COCOeval(cocoGt, cocoDt, 'bbox')\n",
    "    cocoEval.params.imgIds = [image_ID]\n",
    "    cocoEval.evaluate()\n",
    "    cocoEval.accumulate()\n",
    "    cocoEval.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GENERATED LABELS\")\n",
    "pprint(clean_generated_labels)\n",
    "if (input_mode == \"random\"):\n",
    "    print(\"GROUND TRUTH LABELS\")\n",
    "    pprint(ground_truth_labels)\n",
    "\n",
    "\n",
    "if (input_mode == \"random\"):  \n",
    "    print(\"[4] Top-1 Accuracy SCORE\")\n",
    "    top_one_score = top_one_scores.count(True)/len(top_one_scores)\n",
    "    print(top_one_score)\n",
    "    print(\"[4] Top-5 Accuracy SCORE\")\n",
    "    top_five_score = top_five_scores.count(True)/len(top_five_scores)\n",
    "    print(top_five_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generated_image = io.imread(f\"../output/generated-masks-labels.jpg\")\n",
    "fig, axs = plt.subplots(1, 2,figsize=(20, 10))\n",
    "axs[0].imshow(generated_image, cmap='gray')\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title(\"Generated Image (SAM-OpenClip)\")\n",
    "yolo_image = io.imread(f\"../output/yolo-masks-labels.jpg\")\n",
    "axs[1].imshow(yolo_image, cmap='gray')\n",
    "axs[1].axis('off')\n",
    "axs[1].set_title(\"Generated Image (Yolov8)\")\n",
    "plt.subplots_adjust(wspace=0.1)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
