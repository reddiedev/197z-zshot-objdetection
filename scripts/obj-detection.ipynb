{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEE 197z Project 1 - Zero Shot Object Detection\n",
    "use SAM to perform zero-shot object detection using COCO 2017 val split. \n",
    "\n",
    "*Author: Sean Red Mendoza | 2020-01751 | scmendoza5@up.edu.ph*\n",
    "\n",
    "## Tools/ References\n",
    "- [SegmentAnything](https://github.com/facebookresearch/segment-anything)\n",
    "- [OpenClip](https://github.com/mlfoundations/open_clip)\n",
    "- [Coco 2017 Validation Dataset](https://cocodataset.org/#home)\n",
    "- [roatienza/mlops](https://github.com/roatienza/mlops)\n",
    "- [roatienza/Deep-Learning-Experiments](https://github.com/roatienza/Deep-Learning-Experiments)\n",
    "- [Google Cloud G2 GPU VM (Nvidia L4)](https://cloud.google.com/blog/products/compute/introducing-g2-vms-with-nvidia-l4-gpus)\n",
    "\n",
    "## Goals\n",
    "\n",
    "## Approach\n",
    "\n",
    "## Notes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import sys\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
    "import os\n",
    "import open_clip\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "import json\n",
    "import shutil\n",
    "import re\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "print(\"[setup]: determining CUDA support...\")\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Torchvision version:\", torchvision.__version__)\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean output directory\n",
    "if not os.path.exists(\"../output\"):\n",
    "    os.mkdir(f\"../output\")\n",
    "shutil.rmtree(\"../output/\")\n",
    "os.mkdir(f\"../output/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images\n",
    "This program supports manual entry of input image links or random sampling from the CoCo 2017 Validation Dataset\n",
    "*Please select your desired input method below:*\n",
    "\n",
    "- [1] Input Images\n",
    "- [2] CoCo Images\n",
    "\n",
    "After selecting the input image source, you will be asked to input the following:\n",
    "- [a] List of image URL strings, or\n",
    "- [b] number of random images sourced from CoCo 2017*\n",
    "\n",
    "**a hard limit of 10 images is set to prevent overloading of the system*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_input():\n",
    "    while True:\n",
    "        user_input = input(\"Enter 1 or 2: \")\n",
    "        if user_input in ['1', '2']:\n",
    "            return int(user_input)\n",
    "        else:\n",
    "            print(\"Invalid input. Please enter 1 or 2.\")\n",
    "            \n",
    "def get_valid_number(n):\n",
    "    while True:\n",
    "        user_input = input(\"Enter an integer from 1 to {}: \".format(n))\n",
    "        if user_input.isdigit():\n",
    "            number = int(user_input)\n",
    "            if 1 <= number <= n:\n",
    "                return number\n",
    "        print(\"Invalid input. Please enter an integer from 1 to {}.\".format(n))\n",
    "\n",
    "# input_type = get_valid_input()\n",
    "input_type = 2\n",
    "\n",
    "if input_type == 1:\n",
    "    input_mode = \"manual\"\n",
    "    print(\"You have selected MANUAL_INPUT, please enter the image URLs in the following block\")\n",
    "elif input_type==2:\n",
    "    input_mode = \"random\"\n",
    "    # input_image_count = get_valid_number(10)\n",
    "    input_image_count = 1\n",
    "    print(f\"You have selected RANDOM_INPUT of {input_image_count} images from the CoCo 2017 dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"[0]: loading coco annotations and captions...\")\n",
    "dataDir = '../coco'\n",
    "dataType = 'val2017'\n",
    "annFile = '{}/annotations/instances_{}.json'.format(dataDir, dataType)\n",
    "\n",
    "if not os.path.isfile(annFile):\n",
    "    print(\"please download the coco val2017 instances dataset!\")\n",
    "    print(\"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\")\n",
    "    quit()\n",
    "\n",
    "# initialize COCO api for instance annotations + COCO ground truth api\n",
    "cocoGt = COCO(annFile)\n",
    "annFile = '{}/annotations/captions_{}.json'.format(dataDir, dataType)\n",
    "coco_caps = COCO(annFile)\n",
    "catIDs = cocoGt.getCatIds()\n",
    "cats = cocoGt.loadCats(catIDs)\n",
    "cocoCategories = [cat['name'] for cat in cats]\n",
    "\n",
    "# get all captions\n",
    "annIds = coco_caps.getAnnIds(imgIds=[], catIds=[])\n",
    "anns = coco_caps.loadAnns(annIds)\n",
    "coco_labels_words = []\n",
    "for ann in anns:\n",
    "    words = ann['caption'].split()\n",
    "    for word in words:\n",
    "        clean_word = re.sub(r'[^a-zA-Z0-9\\s]', '', word.lower())\n",
    "        coco_labels_words.append(clean_word)\n",
    "\n",
    "coco_labels_words_values = list(set(coco_labels_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image_IDs = []\n",
    "input_image_catIDs = []\n",
    "input_images_links = []\n",
    "input_image_areas = []\n",
    "input_image_labels = []\n",
    "\n",
    "## RANDOM INPUT\n",
    "def get_random_coco_image(n):\n",
    "    for i in range(n):\n",
    "        # get random coco image\n",
    "        catID = catIDs[np.random.randint(0, len(catIDs))]\n",
    "        catImages = cocoGt.getImgIds(catIds=[catID])\n",
    "        imgID = catImages[np.random.randint(0, len(catImages))]\n",
    "        # build image meta data\n",
    "        img = cocoGt.loadImgs(imgID)[0]\n",
    "        imgArea = img['height']*img['width']\n",
    "        imgURL = img['coco_url']\n",
    "        # fetch ground truth labels from coco\n",
    "        annIds = coco_caps.getAnnIds(imgIds=imgID)\n",
    "        anns = coco_caps.loadAnns(annIds)\n",
    "        ground_truth_labels = list(map(lambda x: x['caption'], anns))\n",
    "        input_image_IDs.append(imgID)\n",
    "        input_image_catIDs.append(catID)\n",
    "        input_images_links.append(imgURL)\n",
    "        input_image_areas.append(imgArea)\n",
    "        input_image_labels.append(ground_truth_labels)\n",
    "\n",
    "if input_type == 2:\n",
    "    random_images = get_random_coco_image(input_image_count)    \n",
    "\n",
    "## MODIFY FOR MANUAL INPUT\n",
    "if input_type == 1:\n",
    "    # input_images_links = ['../images/dog_car.jpg']\n",
    "    input_images_links = ['http://farm7.staticflickr.com/6024/6016274664_ea4ecac20c_z.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_results = []\n",
    "label_results = []\n",
    "top_one_scores = []\n",
    "top_five_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[1]: loading sam model\")\n",
    "sam_checkpoint = os.path.join(\"../checkpoints\", \"sam_vit_h_4b8939.pth\")\n",
    "\n",
    "# Download the file if it does not exist\n",
    "url = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\"\n",
    "if not os.path.isfile(sam_checkpoint):\n",
    "    urllib.request.urlretrieve(url, sam_checkpoint)\n",
    "\n",
    "model_type = \"vit_h\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam = sam.to(device=device)\n",
    "mask_generator = SamAutomaticMaskGenerator(\n",
    "    model=sam,\n",
    "    points_per_side=32,\n",
    "    points_per_batch=128,\n",
    "    pred_iou_thresh=0.8,\n",
    "    box_nms_thresh=0.6,\n",
    "    stability_score_thresh=0.75,\n",
    "    min_mask_region_area=1000,  # Requires open-cv to run post-processing\n",
    "    crop_n_layers=1,\n",
    "    crop_n_points_downscale_factor=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[2]: creating open clip model...\")\n",
    "modelType = 'ViT-L-14'\n",
    "modelDataset = \"laion400m_e32\"\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    modelType, pretrained=modelDataset)\n",
    "tokenizer = open_clip.get_tokenizer(modelType)\n",
    "\n",
    "model = model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../labels/imagenet1000_labels.txt\"\n",
    "url = \"https://gist.githubusercontent.com/yrevar/942d3a0ac09ec9e5eb3a/raw/238f720ff059c1f82f368259d1ca4ffa5dd8f9f5/imagenet1000_clsidx_to_labels.txt\"\n",
    "\n",
    "# Download the file if it does not exist\n",
    "if not os.path.isfile(filename):\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "with open(filename) as f:\n",
    "    idx2label = eval(f.read())\n",
    "\n",
    "raw_imagenet_labels = list(idx2label.values())\n",
    "imagenet_labels = []\n",
    "for imagenet_label in raw_imagenet_labels:\n",
    "    args = imagenet_label.split(\",\")\n",
    "    if len(args)>=2:\n",
    "        for arg in args:\n",
    "            imagenet_labels.append(arg.lower().strip())\n",
    "    else: \n",
    "        imagenet_labels.append(imagenet_label.lower().strip)\n",
    "        \n",
    "text = tokenizer(imagenet_labels)\n",
    "text = text.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        img = np.ones((m.shape[0], m.shape[1], 3))\n",
    "        color_mask = np.random.random((1, 3)).tolist()[0]\n",
    "        for i in range(3):\n",
    "            img[:, :, i] = color_mask[i]\n",
    "        ax.imshow(np.dstack((img, m*0.35)))\n",
    "        \n",
    "def shows_masks_with_labels(anns, labels, probs):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "    for i in range(len(sorted_anns)):\n",
    "        mask = sorted_anns[i]\n",
    "        label = labels[i]\n",
    "        prob = probs[i]\n",
    "        if (prob > 0.5):\n",
    "            x, y, w, h = mask['bbox']\n",
    "            color = np.random.rand(3,)\n",
    "            rect = plt.Rectangle((x, y), w, h, fill=False, color=color)\n",
    "            ax.add_patch(rect)\n",
    "            color = np.append(color, 0.5)\n",
    "            ax.text(x, y, f\"{label} {prob:.2f}\", color='black', fontsize=10, bbox=dict(facecolor=color, alpha=0.5))\n",
    "\n",
    "\n",
    "def filter_masks(raw_masks, area):\n",
    "    if len(raw_masks) == 0:\n",
    "        return\n",
    "    length = len(raw_masks)\n",
    "    sorted_anns = sorted(raw_masks, key=(lambda x: x['area']), reverse=True)\n",
    "    filtered_masks = []\n",
    "    for i in range(length):\n",
    "        mask = sorted_anns[i]\n",
    "        if (mask['area'] < area):\n",
    "            continue\n",
    "        filtered_masks.append(mask)\n",
    "    return filtered_masks\n",
    "\n",
    "\n",
    "def save_mask_images(image, image_index, filtered_masks):\n",
    "    if len(filtered_masks) == 0:\n",
    "        return\n",
    "    length = len(filtered_masks)\n",
    "    sorted_anns = sorted(filtered_masks, key=(lambda x: x['area']), reverse=True)\n",
    "    counter = 1\n",
    "    filtered_masks = []\n",
    "    for i in range(length):\n",
    "        mask = sorted_anns[i]\n",
    "        x, y, w, h = mask['bbox']\n",
    "        x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "        im = image[y:y+h, x:x+w]\n",
    "        plt.figure(figsize=(20, 20))\n",
    "        plt.imshow(im)\n",
    "        plt.axis('off')\n",
    "        plt.savefig(f\"../output/{image_index}/mask-{counter}.jpg\",\n",
    "                    bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "        counter += 1\n",
    "        filtered_masks.append(mask)\n",
    "    return filtered_masks\n",
    "\n",
    "def generate_labels(anns, image_index):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    length = len(anns)\n",
    "    labels = []\n",
    "    probs =  []\n",
    "    for i in range(length):\n",
    "        im = Image.open(f\"../output/{image_index}/mask-{i+1}.jpg\").convert(\"RGB\")\n",
    "        img = preprocess(im).unsqueeze(0)\n",
    "        img = img.to(device)\n",
    "\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            image_features = model.encode_image(img)\n",
    "            text_features = model.encode_text(text)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            text_probs = (100.0 * image_features @\n",
    "                          text_features.T).softmax(dim=-1)\n",
    "\n",
    "        text_prob = np.max(text_probs.cpu().numpy())\n",
    "        index = np.argmax(text_probs.cpu().numpy())\n",
    "        label = model_labels[index]\n",
    "        labels.append(label)\n",
    "        probs.append(text_prob)\n",
    "        print(f\"[{i+1}/{length}]:\",  label, f\"({text_prob*100:.2f}%)\",)\n",
    "        \n",
    "    return labels, probs\n",
    "        \n",
    "\n",
    "def save_labels(anns, labels, probs, imgID, catID):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    length = len(anns)\n",
    "    values = []\n",
    "    for i in range(length):\n",
    "        mask = anns[i]\n",
    "        label = labels[i]\n",
    "        text_prob = probs[i]\n",
    "        values.append(\n",
    "            {\"label\": label, \"area\": mask[\"area\"], \"prob\": text_prob})\n",
    "        result = {'image_id': imgID, 'category_id': catID,\n",
    "                  \"bbox\": mask['bbox'], \"score\": mask['predicted_iou']}\n",
    "        coco_results.append(result)\n",
    "    # generate top 5 labels according to label_accuracy and mask_area\n",
    "    sorted_values = sorted(values, key=lambda x: x['prob'])\n",
    "    payload = sorted_values[:5]\n",
    "    labels = list(map(lambda d: d['label'], payload))\n",
    "    return labels\n",
    "\n",
    "def isTopNAccurate(substrings, string):\n",
    "    isAccurate = False\n",
    "    for substring in substrings:\n",
    "        if substring in string:\n",
    "            isAccurate = True\n",
    "            break\n",
    "    return isAccurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_index in range(len(input_images_links)):\n",
    "    image_link = input_images_links[image_index]\n",
    "    print(image_link)\n",
    "    # create output folder for this image\n",
    "    \n",
    "    image_folder_path = f\"../output/{image_index}\"\n",
    "    if not os.path.exists(image_folder_path):\n",
    "        os.mkdir(image_folder_path)\n",
    "    \n",
    "    image = io.imread(image_link)\n",
    "    height, width = image.shape[:2]\n",
    "    imageArea = height * width\n",
    "    maskArea = 0.03 * imageArea\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f\"../output/{image_index}/source.jpg\",\n",
    "                bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "    \n",
    "    raw_masks = mask_generator.generate(image)\n",
    "    print(f\"({image_index}): generated {len(raw_masks)} masks...\")\n",
    "    filtered_masks = filter_masks(raw_masks, maskArea)\n",
    "    print(f\"({image_index}): masks filtered down to {len(filtered_masks)} masks...\")\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(image)\n",
    "    show_anns(filtered_masks)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f\"../output/{image_index}/generated-masks.jpg\",\n",
    "                bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "    \n",
    "    source_image = io.imread(f\"../output/{image_index}/source.jpg\")\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    axs[0].imshow(source_image, cmap='gray')\n",
    "    axs[0].set_title('Source Image')\n",
    "    axs[0].axis('off')\n",
    "    masked_image = io.imread(f\"../output/{image_index}/generated-masks.jpg\")\n",
    "    axs[1].imshow(masked_image, cmap='gray')\n",
    "    axs[1].set_title('Image with Masks')\n",
    "    axs[1].axis('off')\n",
    "    plt.subplots_adjust(wspace=0.4)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # save mask image files\n",
    "    save_mask_images(image, image_index, filtered_masks)\n",
    "    \n",
    "    print(f\"({image_index}): generating labels...\")\n",
    "    generated_labels, generated_probs = generate_labels(filtered_masks, image_index)\n",
    "    print(\"GENERATED LABELS\")\n",
    "    pprint(generated_labels)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    shows_masks_with_labels(filtered_masks, generated_labels, generated_probs)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f\"../output/{image_index}/generated-masks-labels.jpg\",\n",
    "                bbox_inches='tight', pad_inches=0)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    if input_type == 2:\n",
    "        image_id = input_image_IDs[image_index]\n",
    "        image_category_id = input_image_catIDs[image_index]\n",
    "        print(\"image ID:\", image_id, \"category ID:\", image_category_id)\n",
    "        annIds = coco_caps.getAnnIds(imgIds=image_id)\n",
    "        anns = coco_caps.loadAnns(annIds)\n",
    "        ground_truth_labels = []\n",
    "        ground_truth = []\n",
    "        for ann in anns:\n",
    "            ground_truth_labels.append(ann['caption'])\n",
    "            words = ann['caption'].split()\n",
    "            for word in words:\n",
    "                ground_truth.append(word.lower())\n",
    "        ground_truth_values = list(set(ground_truth))\n",
    "\n",
    "        # for each mask image, annotate using open-clip\n",
    "        save_labels(filtered_masks, generated_labels, generated_probs, image_id, image_category_id)\n",
    "        print(\"GROUND TRUTH LABELS\")\n",
    "        pprint(ground_truth_labels)\n",
    "        ground_truth_string = \" \".join(ground_truth_labels)\n",
    "        \n",
    "        top_one_scores.append(generated_labels[0] in ground_truth_string)\n",
    "        top_five_scores.append(isTopNAccurate(\n",
    "            generated_labels, ground_truth_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (input_type == 2):\n",
    "    print(\"[4]: evaluating image results...\")\n",
    "    print(\"[4] mAPS SCORE\")\n",
    "    json_object = json.dumps(coco_results, indent=4)\n",
    "    with open(\"../output/results.json\", \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "    cocoDt = cocoGt.loadRes(\"../output/results.json\")\n",
    "    cocoEval = COCOeval(cocoGt, cocoDt, 'bbox')\n",
    "    cocoEval.params.imgIds = input_image_IDs\n",
    "    cocoEval.evaluate()\n",
    "    cocoEval.accumulate()\n",
    "    cocoEval.summarize()\n",
    "\n",
    "    print(\"[4] Top-1 Accuracy SCORE\")\n",
    "    top_one_score = top_one_scores.count(True)/len(top_one_scores)\n",
    "    print(top_one_score)\n",
    "    print(\"[4] Top-5 Accuracy SCORE\")\n",
    "    top_five_score = top_five_scores.count(True)/len(top_five_scores)\n",
    "    print(top_five_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
