{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open CLIP Demo\n",
    "\n",
    "This notebook demonstrates how to use Open CLIP (open source version of OpenAI's [CLIP](https://github.com/OpenAI/CLIP)) for zero-shot image classification and image to text generation.\n",
    "\n",
    "See ML Foundation's [OpenClip](https://github.com/mlfoundations/open_clip/blob/main/src/open_clip/model.py).\n",
    "\n",
    "Install open clip and pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install open_clip_torch --upgrade\n",
    "! pip install torch --upgrade\n",
    "! pip install torchvision --upgrade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "import urllib\n",
    "import os\n",
    "import numpy as np\n",
    "import urllib \n",
    "from PIL import Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load open clip model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize ImageNet1k labels\n",
    "\n",
    "Load ImageNet1k labels and then tokenize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"imagenet1000_labels.txt\"\n",
    "url = \"https://gist.githubusercontent.com/yrevar/942d3a0ac09ec9e5eb3a/raw/238f720ff059c1f82f368259d1ca4ffa5dd8f9f5/imagenet1000_clsidx_to_labels.txt\"\n",
    "\n",
    "# Download the file if it does not exist\n",
    "if not os.path.isfile(filename):\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "with open(filename) as f:\n",
    "    idx2label = eval(f.read())\n",
    "\n",
    "imagenet_labels = list(idx2label.values())\n",
    "print(imagenet_labels)\n",
    "print(len(imagenet_labels))\n",
    "text = tokenizer(imagenet_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-short image classification using open clip\n",
    "\n",
    "To try: change the image url to any image url you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = \"wonder_cat.jpg\"\n",
    "#url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/0/0b/Cat_poster_1.jpg/640px-Cat_poster_1.jpg\"\n",
    "filename = input(\"Enter the image filename or url: \")\n",
    "if filename.startswith(\"http\"):\n",
    "    filename = urllib.request.urlopen(filename)\n",
    "img = Image.open(filename)\n",
    "\n",
    "image = preprocess(img).unsqueeze(0)\n",
    "image = image.to(device)\n",
    "text = text.to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "\n",
    "index = np.argmax(text_probs.cpu().numpy())\n",
    "print(\"Label:\", imagenet_labels[index])  # prints: tabby, tabby cat\n",
    "# Display the loaded image on notebook.\n",
    "display(img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Text using CoCa\n",
    "\n",
    "Load open clip model trained on [CoCa](https://arxiv.org/ba/2205.01917)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, transform = open_clip.create_model_and_transforms(\n",
    "  model_name=\"coca_ViT-L-14\",\n",
    "  pretrained=\"mscoco_finetuned_laion2B-s13B-b90k\"\n",
    ")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the text for the given image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = img.convert(\"RGB\")\n",
    "im = transform(im).unsqueeze(0)\n",
    "im = im.to(device)\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "  generated = model.generate(im)\n",
    "\n",
    "print(open_clip.decode(generated[0]).split(\"<end_of_text>\")[0].replace(\"<start_of_text>\", \"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
